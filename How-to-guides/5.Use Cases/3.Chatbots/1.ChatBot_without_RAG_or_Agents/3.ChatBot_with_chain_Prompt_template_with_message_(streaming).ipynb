{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.environ.get('LANGCHAIN_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"ChatBot_with_chain_Prompt_template\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Prompt\n",
    "\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n",
    "\n",
    "First, let's add in a system message. To do this, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the chain\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this slightly changes the input type - rather than pass in a list of messages, we are now passing in a dictionary with a messages key where that contains a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola, Bob! ¿Cómo puedo ayudarte hoy?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 33, 'total_tokens': 44}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-249a6a36-4c84-4cc1-944f-1f2f517c27f7-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message History\n",
    "\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define message store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get session history\n",
    "This function is expected to take in a session_id and return a Message History object. This session_id is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a unique session ID\n",
    "import uuid\n",
    "\n",
    "def generate_session_id() -> str:\n",
    "    return str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dynamic session ID\n",
    "session_id_1 = generate_session_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Config\n",
    "\n",
    "We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a session_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\"configurable\": {\"session_id\": session_id_1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola, Todd! ¿Cómo puedo ayudarte hoy?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 33, 'total_tokens': 44}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_f3db212e1c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5e671cb9-0c01-4150-b4e6-2ecaf808b05f-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='आपका नाम टॉड है।', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 56, 'total_tokens': 64}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-af934fd9-b45c-491b-82a1-5f87e88cda68-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What was my last question?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your last question was, \"What\\'s my name?\"', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 78, 'total_tokens': 88}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-af9e9980-138d-449c-8bd8-95b140f090c7-0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your last two questions were:\\n\\n1. \"What was my last question?\"\\n2. \"What’s my name?\"', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 104, 'total_tokens': 127}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-bacec727-7799-4c8c-a9e5-7d118e2dd62a-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me my last 2 questions?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your last two questions were:\\n\\n1. \"What was my last question?\"\\n2. \"What’s my name?\"', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 143, 'total_tokens': 166}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_db4a9208a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-69038af2-0fb3-49c6-9e74-a2fb233c6e2d-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me my last 2 questions?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I can only recall the context of our current conversation, which includes your last few questions. I don’t have the capability to access or remember previous interactions beyond this session. If you have specific questions or topics you'd like to revisit, feel free to ask!\", response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 182, 'total_tokens': 233}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-33a8e088-d4de-45de-9c2f-c7393246f30e-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me my last 70 questions?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dynamic session ID\n",
    "session_id_2 = generate_session_id()\n",
    "config_2 = {\"configurable\": {\"session_id\": session_id_2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Todd. Here's a joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 266, 'total_tokens': 289}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-01d4514e-59ed-4b09-960d-bdccef496ccb-0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name? Tell me a joke\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Todd. Here's a joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming final outputs\n",
    "The .stream method will by default stream each key in a sequence.\n",
    "\n",
    "Note that here only the \"answer\" key is streamed token-by-token, as the other components-- such as retrieval-- do not support token-level streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = with_message_history.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name? Tell me a joke\")], \"language\": \"Engling\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='Your' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' name' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' is' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' Todd' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='.' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' Here' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='’s' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' a' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' joke' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' for' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' you' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=':\\n\\n' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='Why' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' did' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' the' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' scare' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='crow' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' win' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' an' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' award' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='?\\n\\n' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='Because' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' he' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' was' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' outstanding' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' in' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' his' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content=' field' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='!' id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'content='' response_metadata={'finish_reason': 'stop'} id='run-8a0873c9-cb69-4f27-8bdc-a3de47383b40'"
     ]
    }
   ],
   "source": [
    "for chunk in stream:\n",
    "    print(f\"{chunk}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = with_message_history.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name? Tell me a joke\")], \"language\": \"Engling\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Your| name| is| Todd|.| Here|’s| another| joke| for| you|:\n",
      "\n",
      "|Why| did| the| bicycle| fall| over|?\n",
      "\n",
      "|Because| it| was| two|-t|ired|!||"
     ]
    }
   ],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = with_message_history.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name? Tell me a joke\")], \"language\": \"Engling\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Todd. Here’s a joke for you:\n",
      "\n",
      "Why did the math book look sad?\n",
      "\n",
      "Because it had too many problems!"
     ]
    }
   ],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
