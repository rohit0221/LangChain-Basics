{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.environ.get('LANGCHAIN_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"ChatBot_with_chain_Prompt_template\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Prompt\n",
    "\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n",
    "\n",
    "First, let's add in a system message. To do this, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the chain\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this slightly changes the input type - rather than pass in a list of messages, we are now passing in a dictionary with a messages key where that contains a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola, Bob! ¿Cómo puedo ayudarte hoy?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 33, 'total_tokens': 44}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-aecba663-d394-4eba-a64b-df2413af37a2-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message History\n",
    "\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define message store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get session history\n",
    "This function is expected to take in a session_id and return a Message History object. This session_id is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a unique session ID\n",
    "import uuid\n",
    "\n",
    "def generate_session_id() -> str:\n",
    "    return str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dynamic session ID\n",
    "session_id_1 = generate_session_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Config\n",
    "\n",
    "We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a session_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\"configurable\": {\"session_id\": session_id_1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola, Todd! ¿En qué puedo ayudarte hoy?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 33, 'total_tokens': 45}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-429801ac-2e68-4275-9eee-ee76461eb6b3-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='आपका नाम टॉड है।', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 57, 'total_tokens': 65}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-3f574923-74d6-4047-b4f8-f6eab04aa7b1-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What was my last question?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your last question was, \"What\\'s my name?\"', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 79, 'total_tokens': 89}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-6905cdac-a800-4075-9b23-6665d5a3a987-0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your last two questions were:\\n\\n1. \"What was my last question?\"\\n2. \"What’s my name?\"', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 105, 'total_tokens': 128}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-390de423-f8c3-40aa-9dc6-1f5127409d93-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me my last 2 questions?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your last two questions were:\\n\\n1. \"Tell me my last 2 questions?\"\\n2. \"What was my last question?\"', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 144, 'total_tokens': 170}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-1a887ba7-aeb8-417a-9c90-9909ef931f57-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me my last 2 questions?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I can only provide the context of our current conversation, which includes the questions you\\'ve asked so far. Here are your most recent questions:\\n\\n1. \"Tell me my last 2 questions?\"\\n2. \"What was my last question?\"\\n3. \"What’s my name?\"\\n4. \"What was my last question?\"\\n5. \"Tell me my last 2 questions?\"\\n\\nUnfortunately, I don\\'t have the ability to retrieve a longer history of questions beyond this conversation. If you have more questions or need assistance with something else, feel free to ask!', response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 186, 'total_tokens': 296}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-0acbbda3-bd50-47c7-9dd1-358a0af074a5-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me my last 70 questions?\")], \"language\": \"English\"},\n",
    "    config=config_1,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dynamic session ID\n",
    "session_id_2 = generate_session_id()\n",
    "config_2 = {\"configurable\": {\"session_id\": session_id_2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='मुझे आपका नाम नहीं पता। अगर आप चाहें तो मुझे अपना नाम बता सकते हैं!', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 33, 'total_tokens': 52}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_db4a9208a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-fa81adfd-2fad-4473-baab-6781b104bd33-0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config_2,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming final outputs\n",
    "The .stream method will by default stream each key in a sequence.\n",
    "\n",
    "Note that here only the \"answer\" key is streamed token-by-token, as the other components-- such as retrieval-- do not support token-level streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = with_message_history.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content='म' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content='ुझे' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' आपका' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' नाम' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' नहीं' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' पता' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content='।' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' क्या' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' आप' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' मुझे' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' अपना' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' नाम' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' बता' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' सकते' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content=' हैं' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content='?' id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n",
      "content='' response_metadata={'finish_reason': 'stop'} id='run-7dcec52f-f312-4eba-9baf-a15f5b2cb5f0'\n"
     ]
    }
   ],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
