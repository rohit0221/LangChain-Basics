{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents\n",
    "We can use the YouTubeLoader to load transcripts of a few LangChain videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Add some additional metadata: what year the video was published\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_year\"] = int(\n",
    "        datetime.datetime.strptime(\n",
    "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
    "        ).strftime(\"%Y\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the titles of the videos we've loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the metadata associated with each video. We can see that each document also has a title, view count, publication date, and length:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'HAn9vnJy6S4',\n",
       " 'title': 'OpenGPTs',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 9253,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
       " 'publish_date': '2024-01-31 00:00:00',\n",
       " 'length': 1530,\n",
       " 'author': 'LangChain',\n",
       " 'publish_year': 2024}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a sample from a document's contents:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be us\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing documents\n",
    "Whenever we perform retrieval we need to create an index of documents that we can query. We'll use a vector store to index our documents, and we'll chunk them first to make our retrievals more concise and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunked_docs,\n",
    "    embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval without query analysis\n",
    "We can perform similarity search on a user question directly to find chunks relevant to the question:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works pretty well! Our first result is quite relevant to the question.\n",
    "\n",
    "What if we wanted to search for results from a specific time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "2024-01-31 00:00:00\n",
      "hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first result is from 2024 (despite us asking for videos from 2023), and not very relevant to the input. Since we're just searching against document contents, there's no way for the results to be filtered on any document attributes.\n",
    "\n",
    "This is just one failure mode that can arise. Let's now take a look at how a basic form of query analysis can fix it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query analysis\n",
    "We can use query analysis to improve the results of retrieval. \n",
    "\n",
    "This will involve defining a query schema that contains some date filters and use a function-calling model to convert a user question into a structured queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query schema (Data Model)\n",
    "In this case we'll have explicit min and max attributes for publication date so that it can be filtered on.\n",
    "\n",
    "We'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a sub_queries field that contains more narrow questions derived from the top level question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "sub_queries_description = \"\"\"\\\n",
    "If the original question contains multiple distinct sub-questions, \\\n",
    "or if there are more generic questions that would be helpful to answer in \\\n",
    "order to answer the original question, write a list of all relevant sub-questions. \\\n",
    "Make sure this list is comprehensive and covers all parts of the original question. \\\n",
    "It's ok if there's redundancy in the sub-questions. \\\n",
    "Make sure the sub-questions are as narrowly focused as possible.\"\"\"\n",
    "\n",
    "\n",
    "class Search(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"Primary similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    sub_queries: List[str] = Field(\n",
    "        default_factory=list, description=sub_queries_description\n",
    "    )\n",
    "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query generation\n",
    "To convert user questions to structured queries we'll make use of OpenAI's tool-calling API. \n",
    "Specifically we'll use the new ChatModel.with_structured_output() constructor to handle passing the schema to the model and parsing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"examples\", optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='difference between web voyager and reflection agents', sub_queries=['what is web voyager', 'what are reflection agents', 'do both web voyager and reflection agents use langgraph?'], publish_year=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    \"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding examples and tuning the prompt\n",
    "This works pretty well, but we probably want it to decompose the question even further to separate the queries about Web Voyager and Reflection Agents.\n",
    "\n",
    "To tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What's chat langchain, is it a langchain template?\"\n",
    "query = Search(\n",
    "    query=\"What is chat langchain and is it a langchain template?\",\n",
    "    sub_queries=[\"What is chat langchain\", \"What is a langchain template\"],\n",
    ")\n",
    "examples.append({\"input\": question, \"tool_calls\": [query]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to build multi-agent system and stream intermediate steps from it\"\n",
    "query = Search(\n",
    "    query=\"How to build multi-agent system and stream intermediate steps from it\",\n",
    "    sub_queries=[\n",
    "        \"How to build multi-agent system\",\n",
    "        \"How to stream intermediate steps from multi-agent system\",\n",
    "        \"How to stream intermediate steps\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples.append({\"input\": question, \"tool_calls\": [query]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"LangChain agents vs LangGraph?\"\n",
    "query = Search(\n",
    "    query=\"What's the difference between LangChain agents and LangGraph? How do you deploy them?\",\n",
    "    sub_queries=[\n",
    "        \"What are LangChain agents\",\n",
    "        \"What is LangGraph\",\n",
    "        \"How do you deploy LangChain agents\",\n",
    "        \"How do you deploy LangGraph\",\n",
    "    ],\n",
    ")\n",
    "examples.append({\"input\": question, \"tool_calls\": [query]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. \n",
    "We'll create a tool_example_to_messages helper function to handle this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages\n",
    "\n",
    "\n",
    "example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "query_analyzer_with_examples = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt.partial(examples=example_msgs)\n",
    "    | structured_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_analyzer_with_examples.invoke(\n",
    "    \"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some more prompt engineering and tuning of our examples we could improve query generation even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query=\"What's the difference between web voyager and reflection agents? Do both use langgraph?\", sub_queries=['What is web voyager', 'What are reflection agents', 'Do web voyager and reflection agents use langgraph?'], publish_year=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Search"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this in retrieval now?!\n",
    "### Define a retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: using ```ainvoke```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "async def custom_chain(question):\n",
    "    response = await query_analyzer_with_examples.ainvoke(question)\n",
    "    docs = []\n",
    "    for query in response.sub_queries:\n",
    "        new_docs = await retriever.ainvoke(query)\n",
    "        docs.extend(new_docs)\n",
    "    # You probably want to think about reranking or deduplicating documents here\n",
    "    # But that is a separate topic\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"hey folks I'm Eric from Lang chain and today we're going to be building a search enabled chatbot with EXA which launched today um let's get started uh to begin let's go through the main pieces of software that we're going to be using um to start we're going to be using Lang chain um Lang chain is a framework for developing llm powered applications um it's the company I work at and it's the framework we'll be using we'll be using the python library but we also offer JavaScript library um for folks building in that language um the second thing we're going to be using is EXA EXA is a llm focus search engine which allows us to retrieve results uh from the web to provide additional context for our uh generation chatbot um before giving you your results uh Lang Smith is what we're going to be using for debugging and observability today it is the first party um observability tool built by the us at Lang chain um we're going to be using it to inspect some traces and we're also going to be using um the hosted Lang serve product which is part of Lang Smith um in order to host our application in the end um and uh we're also going to be using Lang serve kind of as previously mentioned uh the open it is an open source package um that allows you to host your chains as rest endpoints um and uh we're going to be using the hosted one as well within Lang Smith uh let's dive it so first we're going to be building our chain in a jupyter notebook then we're going to be porting that over to Lang serve as a rest endpoint we're going to be uh playing with that in the playground that's provided there and last but not least we're going to be hosting that in Lang serve in order to um get it accessible from anyone on the internet um so let's dive in so for our notebook um I'm just going to be using the Jupiter notebook in my vs code instance um first we're going to want to install some of our dependencies um as some of you may have noticed we're starting to split out the package into um\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"will try to detect uh the defined input type for our input over here I think it got a little bit confused because the input of our runnable pass through and our retrieval chain um could be something other than a string um or some sort of formatted dictionary and so uh we had to Define our input type as a string if you want to pass in uh multiple um pieces of the input you are welcome to Define this as like a pedantic base model um which we can play with in a little bit if we want um so here let's try um when is the best time to visit Japan um and we can see uh in the link of playground it actually will show us kind of the individual steps that it's running as part of this um so right now it's kind of running through the retrieval step before passing it all through to the llm um it's taking a little bit of time so we can check our stack Trace right now it is telling me that I am using a wrong API key um so let's try adding my API keys to my environment again and running Lang chain serve again um and then we can try this again and that looks to be going better so we can see that it ran the intermediate steps for retrieval um and then in the playground we actually get streaming output as well um which is pretty good uh so here we get kind of a guide for uh why it might depend on several factors let's ask specifically for a ski trip um and see if it gives us a more concrete answer and here it does and it starts planning our trip as well figuring out which ski area we're going to go to um the other nice thing about Lan serve is even when using it hosted uh locally like this um we will actually see these traces show up in lsmith as well um so here because the playground is using streaming output the only difference is the output is going to be um kind of uh some sort of chunked output but Lang Smith handles that pretty well um in terms of displaying that to you uh and overall our Trace actually looks pretty similar um where we can see kind of the that's actually\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different variants of multi-agent workflows that we're going to use laying graph to create so the first one is what we're calling multi-agent collaboration and so this is largely defined as you have multiple agents working on the same state of messages so when you think about multiple agents collaborating they can either kind of like share State between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other and so in a multi-agent collaboration they share the state and so specifically this this example that we've set up it has two different agents and these are basically prompts plus llms so there's a prompts plus an llm for a researcher and there's a prompt plus an llm for a chart generator they each have a few different tools that they can call but basically the way that it works is we first call the researcher uh node or the researcher agent we get a message and then from there there's three different ways that it can go one if the researcher says to call a function then we call that tool and then we go back to the researcher two if the researcher says a message that that says final answer so we in the prompt we say like if you're done say final answer so if the researcher says final answer then it returns to the user and then three if it just sends a normal message there's no tool calls and there's no final answer then we let the chart generator take a look at the state that's accumulated um and kind of uh respond after that so we're going to we're going to walk through what this looks like um we're first going to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"to the next one which is the chart generator so it calls the chart generator under the hood that calls open AI um we can see that down here we get a function call and this is running python code so it prints this out calls the python reppel um uh we can see that it runs this um the chart generator then goes um and the chart generator says uh you know here's the line graph see the chart above the researcher then goes and it says final answer so now that there's final answer it's finished and that's basically what happened um so that's the multi-agent collaboration bit the the kind of like specific thing about this that's interesting is it has This Global state of messages that each llm sees and a penss to so it keeps on adding to this over time and so on one hand this is good because it allows each agent to see kind of like exactly the other steps that the other one is doing and that's why we called it collaboration it's very collaborative the other version of this is that you have agents that have their own independent scratch pads um and so we'll take a look at the next multi-agent example that we have which is this agent supervisor so here we have the supervisor which basically routes between different independent agents so these agents will be Lang chain agents um so they'll basically be an llm that's run with the agent executor in a loop the llm decides what to do um the agent executive then calls the tool goes back to the llm calls another tool goes back to the llm and then then finishes and uh then there's basically the supervisor the supervisor has its own list of messages and essentially what it will do is it will call agent One agent one will go do its work but it will return only the final answer only that final answer is then added the supervisor only sees his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"complex and that's where lsmith comes in handy so the deployed version we have hooked up to a project in lsmith and so we can click on here we have this open gpts example project um and so if I go in here and I click on a trace I can see exactly what's kind of like going on under the hood um and so here um this is if you remember this is actually the system message that I added um when I configured the pirate chatbot um and so this is a system message this is what I said and is the response and so I can track it and so I can also leave when I when I left the thumbs up and thumbs down um I can track that here um and so I believe yeah so here I left the thumbs up on this was this was the weatherman um I I got back this response if we click in what's here I think yeah so this is when we gave it access to the Search tool um and so you can see like exactly what's going on um this is a pretty simple agent because it just responded um but I can see the feedback here as well it's at the top level so I can see the feedback here as well I have a user score of one um and so yeah Lang Smith is a whole separate concept but I just want to point out that openg gpts if you deploy it it's integrated with Lang Smith um if you need Lang Smith access shoot me a DM on Twitter or LinkedIn and can get you access to that that's pretty much it for what I wanted to cover um hopefully this gives you a good sense of both how to use the front end the the research example as well as how to configure the back end um so I think I think the important thing to note that I would highlight is that you can put any different type of architecture behind here right now all the architectures the three different architectures we have the assistant architecture the rag architecture and the chatbot architecture they all use this message graph implementation which passes around a list of messages and I really like this because it's a single common uh kind of like interface that'll make it easy to add on\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different variants of multi-agent workflows that we're going to use laying graph to create so the first one is what we're calling multi-agent collaboration and so this is largely defined as you have multiple agents working on the same state of messages so when you think about multiple agents collaborating they can either kind of like share State between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other and so in a multi-agent collaboration they share the state and so specifically this this example that we've set up it has two different agents and these are basically prompts plus llms so there's a prompts plus an llm for a researcher and there's a prompt plus an llm for a chart generator they each have a few different tools that they can call but basically the way that it works is we first call the researcher uh node or the researcher agent we get a message and then from there there's three different ways that it can go one if the researcher says to call a function then we call that tool and then we go back to the researcher two if the researcher says a message that that says final answer so we in the prompt we say like if you're done say final answer so if the researcher says final answer then it returns to the user and then three if it just sends a normal message there's no tool calls and there's no final answer then we let the chart generator take a look at the state that's accumulated um and kind of uh respond after that so we're going to we're going to walk through what this looks like um we're first going to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await custom_chain.ainvoke(\"what's the difference between web voyager and reflection agents? do both use langgraph?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method-2\n",
    "\n",
    "Using a custom master chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain the query analyzer and the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "    query_analyzer_with_examples\n",
    "    | (lambda result: result.sub_queries)  # Access sub_queries as an attribute\n",
    "    | (lambda sub_queries: [retriever.invoke(sub_query) for sub_query in sub_queries])  # Retrieve results for each subquery\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"hey folks I'm Eric from Lang chain and today we're going to be building a search enabled chatbot with EXA which launched today um let's get started uh to begin let's go through the main pieces of software that we're going to be using um to start we're going to be using Lang chain um Lang chain is a framework for developing llm powered applications um it's the company I work at and it's the framework we'll be using we'll be using the python library but we also offer JavaScript library um for folks building in that language um the second thing we're going to be using is EXA EXA is a llm focus search engine which allows us to retrieve results uh from the web to provide additional context for our uh generation chatbot um before giving you your results uh Lang Smith is what we're going to be using for debugging and observability today it is the first party um observability tool built by the us at Lang chain um we're going to be using it to inspect some traces and we're also going to be using um the hosted Lang serve product which is part of Lang Smith um in order to host our application in the end um and uh we're also going to be using Lang serve kind of as previously mentioned uh the open it is an open source package um that allows you to host your chains as rest endpoints um and uh we're going to be using the hosted one as well within Lang Smith uh let's dive it so first we're going to be building our chain in a jupyter notebook then we're going to be porting that over to Lang serve as a rest endpoint we're going to be uh playing with that in the playground that's provided there and last but not least we're going to be hosting that in Lang serve in order to um get it accessible from anyone on the internet um so let's dive in so for our notebook um I'm just going to be using the Jupiter notebook in my vs code instance um first we're going to want to install some of our dependencies um as some of you may have noticed we're starting to split out the package into um\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"will try to detect uh the defined input type for our input over here I think it got a little bit confused because the input of our runnable pass through and our retrieval chain um could be something other than a string um or some sort of formatted dictionary and so uh we had to Define our input type as a string if you want to pass in uh multiple um pieces of the input you are welcome to Define this as like a pedantic base model um which we can play with in a little bit if we want um so here let's try um when is the best time to visit Japan um and we can see uh in the link of playground it actually will show us kind of the individual steps that it's running as part of this um so right now it's kind of running through the retrieval step before passing it all through to the llm um it's taking a little bit of time so we can check our stack Trace right now it is telling me that I am using a wrong API key um so let's try adding my API keys to my environment again and running Lang chain serve again um and then we can try this again and that looks to be going better so we can see that it ran the intermediate steps for retrieval um and then in the playground we actually get streaming output as well um which is pretty good uh so here we get kind of a guide for uh why it might depend on several factors let's ask specifically for a ski trip um and see if it gives us a more concrete answer and here it does and it starts planning our trip as well figuring out which ski area we're going to go to um the other nice thing about Lan serve is even when using it hosted uh locally like this um we will actually see these traces show up in lsmith as well um so here because the playground is using streaming output the only difference is the output is going to be um kind of uh some sort of chunked output but Lang Smith handles that pretty well um in terms of displaying that to you uh and overall our Trace actually looks pretty similar um where we can see kind of the that's actually\")],\n",
       " [Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different variants of multi-agent workflows that we're going to use laying graph to create so the first one is what we're calling multi-agent collaboration and so this is largely defined as you have multiple agents working on the same state of messages so when you think about multiple agents collaborating they can either kind of like share State between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other and so in a multi-agent collaboration they share the state and so specifically this this example that we've set up it has two different agents and these are basically prompts plus llms so there's a prompts plus an llm for a researcher and there's a prompt plus an llm for a chart generator they each have a few different tools that they can call but basically the way that it works is we first call the researcher uh node or the researcher agent we get a message and then from there there's three different ways that it can go one if the researcher says to call a function then we call that tool and then we go back to the researcher two if the researcher says a message that that says final answer so we in the prompt we say like if you're done say final answer so if the researcher says final answer then it returns to the user and then three if it just sends a normal message there's no tool calls and there's no final answer then we let the chart generator take a look at the state that's accumulated um and kind of uh respond after that so we're going to we're going to walk through what this looks like um we're first going to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"to the next one which is the chart generator so it calls the chart generator under the hood that calls open AI um we can see that down here we get a function call and this is running python code so it prints this out calls the python reppel um uh we can see that it runs this um the chart generator then goes um and the chart generator says uh you know here's the line graph see the chart above the researcher then goes and it says final answer so now that there's final answer it's finished and that's basically what happened um so that's the multi-agent collaboration bit the the kind of like specific thing about this that's interesting is it has This Global state of messages that each llm sees and a penss to so it keeps on adding to this over time and so on one hand this is good because it allows each agent to see kind of like exactly the other steps that the other one is doing and that's why we called it collaboration it's very collaborative the other version of this is that you have agents that have their own independent scratch pads um and so we'll take a look at the next multi-agent example that we have which is this agent supervisor so here we have the supervisor which basically routes between different independent agents so these agents will be Lang chain agents um so they'll basically be an llm that's run with the agent executor in a loop the llm decides what to do um the agent executive then calls the tool goes back to the llm calls another tool goes back to the llm and then then finishes and uh then there's basically the supervisor the supervisor has its own list of messages and essentially what it will do is it will call agent One agent one will go do its work but it will return only the final answer only that final answer is then added the supervisor only sees his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's\")],\n",
       " [Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"complex and that's where lsmith comes in handy so the deployed version we have hooked up to a project in lsmith and so we can click on here we have this open gpts example project um and so if I go in here and I click on a trace I can see exactly what's kind of like going on under the hood um and so here um this is if you remember this is actually the system message that I added um when I configured the pirate chatbot um and so this is a system message this is what I said and is the response and so I can track it and so I can also leave when I when I left the thumbs up and thumbs down um I can track that here um and so I believe yeah so here I left the thumbs up on this was this was the weatherman um I I got back this response if we click in what's here I think yeah so this is when we gave it access to the Search tool um and so you can see like exactly what's going on um this is a pretty simple agent because it just responded um but I can see the feedback here as well it's at the top level so I can see the feedback here as well I have a user score of one um and so yeah Lang Smith is a whole separate concept but I just want to point out that openg gpts if you deploy it it's integrated with Lang Smith um if you need Lang Smith access shoot me a DM on Twitter or LinkedIn and can get you access to that that's pretty much it for what I wanted to cover um hopefully this gives you a good sense of both how to use the front end the the research example as well as how to configure the back end um so I think I think the important thing to note that I would highlight is that you can put any different type of architecture behind here right now all the architectures the three different architectures we have the assistant architecture the rag architecture and the chatbot architecture they all use this message graph implementation which passes around a list of messages and I really like this because it's a single common uh kind of like interface that'll make it easy to add on\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different variants of multi-agent workflows that we're going to use laying graph to create so the first one is what we're calling multi-agent collaboration and so this is largely defined as you have multiple agents working on the same state of messages so when you think about multiple agents collaborating they can either kind of like share State between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other and so in a multi-agent collaboration they share the state and so specifically this this example that we've set up it has two different agents and these are basically prompts plus llms so there's a prompts plus an llm for a researcher and there's a prompt plus an llm for a chart generator they each have a few different tools that they can call but basically the way that it works is we first call the researcher uh node or the researcher agent we get a message and then from there there's three different ways that it can go one if the researcher says to call a function then we call that tool and then we go back to the researcher two if the researcher says a message that that says final answer so we in the prompt we say like if you're done say final answer so if the researcher says final answer then it returns to the user and then three if it just sends a normal message there's no tool calls and there's no final answer then we let the chart generator take a look at the state that's accumulated um and kind of uh respond after that so we're going to we're going to walk through what this looks like um we're first going to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good\")]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = full_chain.invoke(\"what's the difference between web voyager and reflection agents? do both use langgraph?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a single list of docs\n",
    "Right now as you see the docs are lists of lists.\n",
    "For each query there is a list of docs so we have 3 lists of docs.append\n",
    "\n",
    "We can flatten it into a single list of docs if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Assume `docs` is your current list of lists\n",
    "flattened_docs = list(chain.from_iterable(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"hey folks I'm Eric from Lang chain and today we're going to be building a search enabled chatbot with EXA which launched today um let's get started uh to begin let's go through the main pieces of software that we're going to be using um to start we're going to be using Lang chain um Lang chain is a framework for developing llm powered applications um it's the company I work at and it's the framework we'll be using we'll be using the python library but we also offer JavaScript library um for folks building in that language um the second thing we're going to be using is EXA EXA is a llm focus search engine which allows us to retrieve results uh from the web to provide additional context for our uh generation chatbot um before giving you your results uh Lang Smith is what we're going to be using for debugging and observability today it is the first party um observability tool built by the us at Lang chain um we're going to be using it to inspect some traces and we're also going to be using um the hosted Lang serve product which is part of Lang Smith um in order to host our application in the end um and uh we're also going to be using Lang serve kind of as previously mentioned uh the open it is an open source package um that allows you to host your chains as rest endpoints um and uh we're going to be using the hosted one as well within Lang Smith uh let's dive it so first we're going to be building our chain in a jupyter notebook then we're going to be porting that over to Lang serve as a rest endpoint we're going to be uh playing with that in the playground that's provided there and last but not least we're going to be hosting that in Lang serve in order to um get it accessible from anyone on the internet um so let's dive in so for our notebook um I'm just going to be using the Jupiter notebook in my vs code instance um first we're going to want to install some of our dependencies um as some of you may have noticed we're starting to split out the package into um\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"will try to detect uh the defined input type for our input over here I think it got a little bit confused because the input of our runnable pass through and our retrieval chain um could be something other than a string um or some sort of formatted dictionary and so uh we had to Define our input type as a string if you want to pass in uh multiple um pieces of the input you are welcome to Define this as like a pedantic base model um which we can play with in a little bit if we want um so here let's try um when is the best time to visit Japan um and we can see uh in the link of playground it actually will show us kind of the individual steps that it's running as part of this um so right now it's kind of running through the retrieval step before passing it all through to the llm um it's taking a little bit of time so we can check our stack Trace right now it is telling me that I am using a wrong API key um so let's try adding my API keys to my environment again and running Lang chain serve again um and then we can try this again and that looks to be going better so we can see that it ran the intermediate steps for retrieval um and then in the playground we actually get streaming output as well um which is pretty good uh so here we get kind of a guide for uh why it might depend on several factors let's ask specifically for a ski trip um and see if it gives us a more concrete answer and here it does and it starts planning our trip as well figuring out which ski area we're going to go to um the other nice thing about Lan serve is even when using it hosted uh locally like this um we will actually see these traces show up in lsmith as well um so here because the playground is using streaming output the only difference is the output is going to be um kind of uh some sort of chunked output but Lang Smith handles that pretty well um in terms of displaying that to you uh and overall our Trace actually looks pretty similar um where we can see kind of the that's actually\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different variants of multi-agent workflows that we're going to use laying graph to create so the first one is what we're calling multi-agent collaboration and so this is largely defined as you have multiple agents working on the same state of messages so when you think about multiple agents collaborating they can either kind of like share State between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other and so in a multi-agent collaboration they share the state and so specifically this this example that we've set up it has two different agents and these are basically prompts plus llms so there's a prompts plus an llm for a researcher and there's a prompt plus an llm for a chart generator they each have a few different tools that they can call but basically the way that it works is we first call the researcher uh node or the researcher agent we get a message and then from there there's three different ways that it can go one if the researcher says to call a function then we call that tool and then we go back to the researcher two if the researcher says a message that that says final answer so we in the prompt we say like if you're done say final answer so if the researcher says final answer then it returns to the user and then three if it just sends a normal message there's no tool calls and there's no final answer then we let the chart generator take a look at the state that's accumulated um and kind of uh respond after that so we're going to we're going to walk through what this looks like um we're first going to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"to the next one which is the chart generator so it calls the chart generator under the hood that calls open AI um we can see that down here we get a function call and this is running python code so it prints this out calls the python reppel um uh we can see that it runs this um the chart generator then goes um and the chart generator says uh you know here's the line graph see the chart above the researcher then goes and it says final answer so now that there's final answer it's finished and that's basically what happened um so that's the multi-agent collaboration bit the the kind of like specific thing about this that's interesting is it has This Global state of messages that each llm sees and a penss to so it keeps on adding to this over time and so on one hand this is good because it allows each agent to see kind of like exactly the other steps that the other one is doing and that's why we called it collaboration it's very collaborative the other version of this is that you have agents that have their own independent scratch pads um and so we'll take a look at the next multi-agent example that we have which is this agent supervisor so here we have the supervisor which basically routes between different independent agents so these agents will be Lang chain agents um so they'll basically be an llm that's run with the agent executor in a loop the llm decides what to do um the agent executive then calls the tool goes back to the llm calls another tool goes back to the llm and then then finishes and uh then there's basically the supervisor the supervisor has its own list of messages and essentially what it will do is it will call agent One agent one will go do its work but it will return only the final answer only that final answer is then added the supervisor only sees his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"complex and that's where lsmith comes in handy so the deployed version we have hooked up to a project in lsmith and so we can click on here we have this open gpts example project um and so if I go in here and I click on a trace I can see exactly what's kind of like going on under the hood um and so here um this is if you remember this is actually the system message that I added um when I configured the pirate chatbot um and so this is a system message this is what I said and is the response and so I can track it and so I can also leave when I when I left the thumbs up and thumbs down um I can track that here um and so I believe yeah so here I left the thumbs up on this was this was the weatherman um I I got back this response if we click in what's here I think yeah so this is when we gave it access to the Search tool um and so you can see like exactly what's going on um this is a pretty simple agent because it just responded um but I can see the feedback here as well it's at the top level so I can see the feedback here as well I have a user score of one um and so yeah Lang Smith is a whole separate concept but I just want to point out that openg gpts if you deploy it it's integrated with Lang Smith um if you need Lang Smith access shoot me a DM on Twitter or LinkedIn and can get you access to that that's pretty much it for what I wanted to cover um hopefully this gives you a good sense of both how to use the front end the the research example as well as how to configure the back end um so I think I think the important thing to note that I would highlight is that you can put any different type of architecture behind here right now all the architectures the three different architectures we have the assistant architecture the rag architecture and the chatbot architecture they all use this message graph implementation which passes around a list of messages and I really like this because it's a single common uh kind of like interface that'll make it easy to add on\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64260}, page_content=\"time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different variants of multi-agent workflows that we're going to use laying graph to create so the first one is what we're calling multi-agent collaboration and so this is largely defined as you have multiple agents working on the same state of messages so when you think about multiple agents collaborating they can either kind of like share State between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other and so in a multi-agent collaboration they share the state and so specifically this this example that we've set up it has two different agents and these are basically prompts plus llms so there's a prompts plus an llm for a researcher and there's a prompt plus an llm for a chart generator they each have a few different tools that they can call but basically the way that it works is we first call the researcher uh node or the researcher agent we get a message and then from there there's three different ways that it can go one if the researcher says to call a function then we call that tool and then we go back to the researcher two if the researcher says a message that that says final answer so we in the prompt we say like if you're done say final answer so if the researcher says final answer then it returns to the user and then three if it just sends a normal message there's no tool calls and there's no final answer then we let the chart generator take a look at the state that's accumulated um and kind of uh respond after that so we're going to we're going to walk through what this looks like um we're first going to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good\")]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
