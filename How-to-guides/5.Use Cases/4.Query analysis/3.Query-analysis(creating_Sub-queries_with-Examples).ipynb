{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents\n",
    "We can use the YouTubeLoader to load transcripts of a few LangChain videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Add some additional metadata: what year the video was published\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_year\"] = int(\n",
    "        datetime.datetime.strptime(\n",
    "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
    "        ).strftime(\"%Y\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the titles of the videos we've loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the metadata associated with each video. We can see that each document also has a title, view count, publication date, and length:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'HAn9vnJy6S4',\n",
       " 'title': 'OpenGPTs',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 9253,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
       " 'publish_date': '2024-01-31 00:00:00',\n",
       " 'length': 1530,\n",
       " 'author': 'LangChain',\n",
       " 'publish_year': 2024}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a sample from a document's contents:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be us\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing documents\n",
    "Whenever we perform retrieval we need to create an index of documents that we can query. We'll use a vector store to index our documents, and we'll chunk them first to make our retrievals more concise and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunked_docs,\n",
    "    embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval without query analysis\n",
    "We can perform similarity search on a user question directly to find chunks relevant to the question:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works pretty well! Our first result is quite relevant to the question.\n",
    "\n",
    "What if we wanted to search for results from a specific time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "2024-01-31 00:00:00\n",
      "hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first result is from 2024 (despite us asking for videos from 2023), and not very relevant to the input. Since we're just searching against document contents, there's no way for the results to be filtered on any document attributes.\n",
    "\n",
    "This is just one failure mode that can arise. Let's now take a look at how a basic form of query analysis can fix it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query analysis\n",
    "We can use query analysis to improve the results of retrieval. \n",
    "\n",
    "This will involve defining a query schema that contains some date filters and use a function-calling model to convert a user question into a structured queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query schema (Data Model)\n",
    "In this case we'll have explicit min and max attributes for publication date so that it can be filtered on.\n",
    "\n",
    "We'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a sub_queries field that contains more narrow questions derived from the top level question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "sub_queries_description = \"\"\"\\\n",
    "If the original question contains multiple distinct sub-questions, \\\n",
    "or if there are more generic questions that would be helpful to answer in \\\n",
    "order to answer the original question, write a list of all relevant sub-questions. \\\n",
    "Make sure this list is comprehensive and covers all parts of the original question. \\\n",
    "It's ok if there's redundancy in the sub-questions. \\\n",
    "Make sure the sub-questions are as narrowly focused as possible.\"\"\"\n",
    "\n",
    "\n",
    "class Search(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"Primary similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    sub_queries: List[str] = Field(\n",
    "        default_factory=list, description=sub_queries_description\n",
    "    )\n",
    "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query generation\n",
    "To convert user questions to structured queries we'll make use of OpenAI's tool-calling API. \n",
    "Specifically we'll use the new ChatModel.with_structured_output() constructor to handle passing the schema to the model and parsing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"examples\", optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='difference between web voyager and reflection agents', sub_queries=['what is web voyager', 'what are reflection agents', 'do both web voyager and reflection agents use langgraph?'], publish_year=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    \"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding examples and tuning the prompt\n",
    "This works pretty well, but we probably want it to decompose the question even further to separate the queries about Web Voyager and Reflection Agents.\n",
    "\n",
    "To tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What's chat langchain, is it a langchain template?\"\n",
    "query = Search(\n",
    "    query=\"What is chat langchain and is it a langchain template?\",\n",
    "    sub_queries=[\"What is chat langchain\", \"What is a langchain template\"],\n",
    ")\n",
    "examples.append({\"input\": question, \"tool_calls\": [query]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to build multi-agent system and stream intermediate steps from it\"\n",
    "query = Search(\n",
    "    query=\"How to build multi-agent system and stream intermediate steps from it\",\n",
    "    sub_queries=[\n",
    "        \"How to build multi-agent system\",\n",
    "        \"How to stream intermediate steps from multi-agent system\",\n",
    "        \"How to stream intermediate steps\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples.append({\"input\": question, \"tool_calls\": [query]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"LangChain agents vs LangGraph?\"\n",
    "query = Search(\n",
    "    query=\"What's the difference between LangChain agents and LangGraph? How do you deploy them?\",\n",
    "    sub_queries=[\n",
    "        \"What are LangChain agents\",\n",
    "        \"What is LangGraph\",\n",
    "        \"How do you deploy LangChain agents\",\n",
    "        \"How do you deploy LangGraph\",\n",
    "    ],\n",
    ")\n",
    "examples.append({\"input\": question, \"tool_calls\": [query]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. \n",
    "We'll create a tool_example_to_messages helper function to handle this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages\n",
    "\n",
    "\n",
    "example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "query_analyzer_with_examples = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt.partial(examples=example_msgs)\n",
    "    | structured_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_analyzer_with_examples.invoke(\n",
    "    \"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some more prompt engineering and tuning of our examples we could improve query generation even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query=\"What's the difference between web voyager and reflection agents? Do both use langgraph?\", sub_queries=['What is web voyager', 'What are reflection agents', 'Do web voyager and reflection agents use langgraph?'], publish_year=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this in retrieval now?!\n",
    "### Define a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain the query analyzer and the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "    query_analyzer_with_examples\n",
    "    | (lambda result: result.sub_queries)  # Access sub_queries as an attribute\n",
    "    | (lambda sub_queries: [retriever.invoke(sub_query) for sub_query in sub_queries])  # Retrieve results for each subquery\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\")],\n",
       " [Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or\")],\n",
       " [Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks\"),\n",
       "  Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks\")]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = full_chain.invoke(\"what's the difference between web voyager and reflection agents? do both use langgraph?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a single list of docs\n",
    "Right now as you see the docs are lists of lists.\n",
    "For each query there is a list of docs so we have 3 lists of docs.append\n",
    "\n",
    "We can flatten it into a single list of docs if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Assume `docs` is your current list of lists\n",
    "flattened_docs = list(chain.from_iterable(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1530, 'publish_date': '2024-01-31 00:00:00', 'publish_year': 2024, 'source': 'HAn9vnJy6S4', 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'title': 'OpenGPTs', 'view_count': 9253}, page_content=\"first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1950, 'publish_date': '2024-01-26 00:00:00', 'publish_year': 2024, 'source': 'dA1cHGACXCo', 'thumbnail_url': 'https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw', 'title': 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve', 'view_count': 8095}, page_content=\"there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64259}, page_content=\"are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks\"),\n",
       " Document(metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1441, 'publish_date': '2024-01-23 00:00:00', 'publish_year': 2024, 'source': 'hvAPnpSfSGo', 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'title': 'LangGraph: Multi-Agent Workflows', 'view_count': 64237}, page_content=\"are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks\")]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
